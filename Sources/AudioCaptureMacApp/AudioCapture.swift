import Foundation
import AVFoundation
import WebSocketKit
import Vapor
import ScreenCaptureKit

// è®©WebSocketå¯ä»¥åœ¨Setä¸­ä½¿ç”¨
extension WebSocket: @retroactive Hashable {
    public func hash(into hasher: inout Hasher) {
        hasher.combine(ObjectIdentifier(self))
    }
    
    public static func == (lhs: WebSocket, rhs: WebSocket) -> Bool {
        return ObjectIdentifier(lhs) == ObjectIdentifier(rhs)
    }
}

@available(macOS 12.3, *)
class AudioCapture: NSObject, @unchecked Sendable, SCStreamOutput, SCStreamDelegate {
    static let shared = AudioCapture()
    
    private var captureStream: SCStream?
    private var micAudioEngine: AVAudioEngine?
    @MainActor private var webSockets: Set<WebSocket> = []
    private var isCapturing = false
    
    private override init() {
        super.init()
    }
    
    func addWebSocket(_ webSocket: WebSocket) async {
        _ = await MainActor.run {
            webSockets.insert(webSocket)
        }
        
        // ç›‘å¬WebSocketå…³é—­äº‹ä»¶
        webSocket.onClose.whenComplete { [weak self] _ in
            Task {
                await self?.removeWebSocket(webSocket)
            }
        }
    }
    
    func removeWebSocket(_ webSocket: WebSocket) async {
        _ = await MainActor.run {
            webSockets.remove(webSocket)
        }
    }
    
    func startGlobalAudioCapture() async throws {
        guard !isCapturing else { 
            print("âš ï¸ éŸ³é¢‘æ•è·å·²åœ¨è¿è¡Œä¸­")
            return 
        }
        
        print("ğŸ™ï¸ å¼€å§‹å¯åŠ¨éŸ³é¢‘æ•è·...")
        self.isCapturing = true
        
        // æ£€æŸ¥å±å¹•å½•åˆ¶æƒé™
        let hasScreenRecordingPermission = await checkAndRequestScreenRecordingPermission()
        print("ğŸ“º å±å¹•å½•åˆ¶æƒé™çŠ¶æ€: \(hasScreenRecordingPermission ? "âœ… å·²æˆæƒ" : "âŒ æœªæˆæƒ")")
        
        // æ£€æŸ¥éº¦å…‹é£æƒé™
        let hasMicrophonePermission = await checkAndRequestMicrophonePermission()
        print("ğŸ¤ éº¦å…‹é£æƒé™çŠ¶æ€: \(hasMicrophonePermission ? "âœ… å·²æˆæƒ" : "âŒ æœªæˆæƒ")")
        
        // å¯åŠ¨ScreenCaptureKitéŸ³é¢‘æ•è·
        try await startScreenCaptureAudio()
        
        print("âœ… éŸ³é¢‘æ•è·ç³»ç»Ÿå·²å¯åŠ¨")
    }
    
    private func checkAndRequestScreenRecordingPermission() async -> Bool {
        let canRecord = CGPreflightScreenCaptureAccess()
        if !canRecord {
            print("ğŸ” æ­£åœ¨è¯·æ±‚å±å¹•å½•åˆ¶æƒé™...")
            return CGRequestScreenCaptureAccess()
        }
        return true
    }
    
    private func checkAndRequestMicrophonePermission() async -> Bool {
        let status = AVCaptureDevice.authorizationStatus(for: .audio)
        print("ğŸ¤ å½“å‰éº¦å…‹é£æƒé™çŠ¶æ€: \(status.rawValue)")
        
        switch status {
        case .notDetermined:
            print("ğŸ” æ­£åœ¨è¯·æ±‚éº¦å…‹é£æƒé™...")
            return await AVCaptureDevice.requestAccess(for: .audio)
        case .authorized:
            return true
        case .denied, .restricted:
            print("âŒ éº¦å…‹é£æƒé™è¢«æ‹’ç»")
            return false
        @unknown default:
            return false
        }
    }
    
    private func startScreenCaptureAudio() async throws {
        print("ğŸ” è·å–å¯æ•è·å†…å®¹...")
        let availableContent = try await SCShareableContent.excludingDesktopWindows(false, onScreenWindowsOnly: true)
        print("ğŸ“± æ‰¾åˆ° \(availableContent.displays.count) ä¸ªæ˜¾ç¤ºå™¨")
        print("ğŸ“± æ‰¾åˆ° \(availableContent.applications.count) ä¸ªåº”ç”¨ç¨‹åº")
        
        try await setupSystemAudioCapture(availableContent: availableContent)
        try await setupMicrophoneCapture()
    }
    
    private func setupSystemAudioCapture(availableContent: SCShareableContent) async throws {
        guard let display = availableContent.displays.first else { 
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨æ˜¾ç¤ºå™¨")
            return 
        }
        
        print("ğŸ–¥ï¸ ä½¿ç”¨æ˜¾ç¤ºå™¨: \(display.displayID)")
        
        let excludedApps = availableContent.applications.filter { app in
            Bundle.main.bundleIdentifier == app.bundleIdentifier
        }
        
        print("ğŸš« æ’é™¤çš„åº”ç”¨ç¨‹åºæ•°é‡: \(excludedApps.count)")
        
        let filter = SCContentFilter(display: display, excludingApplications: excludedApps, exceptingWindows: [])
        
        let configuration = SCStreamConfiguration()
        if #available(macOS 13.0, *) {
            configuration.capturesAudio = true
            configuration.excludesCurrentProcessAudio = true
            configuration.sampleRate = 16000
            configuration.channelCount = 2
        }
        
        configuration.minimumFrameInterval = CMTime(value: 1, timescale: 60)
        configuration.queueDepth = 8
        
        if #available(macOS 15.0, *) {
            if let defaultMicrophone = AVCaptureDevice.default(for: .audio) {
                configuration.captureMicrophone = true
                configuration.microphoneCaptureDeviceID = defaultMicrophone.uniqueID
                print("ğŸ¤ ä½¿ç”¨å†…ç½®éº¦å…‹é£: \(defaultMicrophone.localizedName)")
            }
        }
        
        captureStream = SCStream(filter: filter, configuration: configuration, delegate: self)
        
        if #available(macOS 13.0, *) {
            try captureStream?.addStreamOutput(self, type: .audio, sampleHandlerQueue: DispatchQueue.global(qos: .userInteractive))
        }
        
        if #available(macOS 15.0, *) {
            try captureStream?.addStreamOutput(self, type: .microphone, sampleHandlerQueue: DispatchQueue.global(qos: .userInteractive))
        }
        
        print("ğŸš€ å¯åŠ¨ScreenCaptureKitæ•è·...")
        try await captureStream?.startCapture()
        print("âœ… ScreenCaptureKitæ•è·å·²å¯åŠ¨")
    }
    
    private func setupMicrophoneCapture() async throws {
        if #unavailable(macOS 15.0) {
            micAudioEngine = AVAudioEngine()
            
            guard let micAudioEngine = micAudioEngine else { return }
            
            let inputNode = micAudioEngine.inputNode
            let inputFormat = inputNode.outputFormat(forBus: 0)
            
            print("ğŸ¤ éº¦å…‹é£æ ¼å¼: \(inputFormat)")
            
            // ä½¿ç”¨ç¡¬ä»¶åŸç”Ÿæ ¼å¼ï¼Œé¿å…æ ¼å¼ä¸åŒ¹é…é—®é¢˜ï¼Œå¢åŠ ç¼“å†²åŒºå¤§å°
            inputNode.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { [weak self] (buffer, time) in
                self?.processMicrophoneAudio(buffer: buffer)
            }
            
            micAudioEngine.prepare()
            try micAudioEngine.start()
            print("âœ… AVAudioEngineéº¦å…‹é£æ•è·å·²å¯åŠ¨")
        }
    }
    
    private func processMicrophoneAudio(buffer: AVAudioPCMBuffer) {
        guard isCapturing,
              let channelData = buffer.floatChannelData?[0] else { 
            return 
        }
        
        let frameCount = Int(buffer.frameLength)
        let audioData = Array(UnsafeBufferPointer(start: channelData, count: frameCount)).map(Double.init)
        
        // è®¡ç®—éŸ³é¢‘å¼ºåº¦ï¼ˆæš‚æ—¶ä¸ä½¿ç”¨ï¼‰
        let _ = sqrt(audioData.map { $0 * $0 }.reduce(0, +) / Double(audioData.count))
        
        // å‘é€éŸ³é¢‘æ•°æ®åˆ°WebSocketå®¢æˆ·ç«¯
        let event = AudioDataEvent(
            id: generateId(),
            payload: AudioPayload(audioType: "mic", data: audioData),
            type: nil,
            wsEventType: "audio-data-event"
        )
        
        sendToAllWebSockets(event: event)
    }
    
    private func processSystemAudio(buffer: AVAudioPCMBuffer) {
        guard isCapturing else { return }
        
        let frameCount = Int(buffer.frameLength)
        let channelCount = Int(buffer.format.channelCount)
        

        
        var audioData: [Double] = []
        
        if channelCount == 1 {
            // å•å£°é“
            guard let channelData = buffer.floatChannelData?[0] else { return }
            audioData = Array(UnsafeBufferPointer(start: channelData, count: frameCount)).map(Double.init)
        } else if channelCount == 2 {
            // ç«‹ä½“å£° - æ··åˆä¸¤ä¸ªå£°é“
            guard let leftChannel = buffer.floatChannelData?[0],
                  let rightChannel = buffer.floatChannelData?[1] else { return }
            
            audioData.reserveCapacity(frameCount)
            for i in 0..<frameCount {
                // å°†å·¦å³å£°é“æ··åˆä¸ºå•å£°é“
                let mixedSample = Double((leftChannel[i] + rightChannel[i]) / 2.0)
                audioData.append(mixedSample)
            }
        } else {
            // å¤šå£°é“ - åªå–ç¬¬ä¸€ä¸ªå£°é“
            guard let channelData = buffer.floatChannelData?[0] else { return }
            audioData = Array(UnsafeBufferPointer(start: channelData, count: frameCount)).map(Double.init)
        }
        
        // è®¡ç®—éŸ³é¢‘å¼ºåº¦ï¼ˆæš‚æ—¶ä¸ä½¿ç”¨ï¼‰
        let _ = sqrt(audioData.map { $0 * $0 }.reduce(0, +) / Double(audioData.count))
        
        let event = AudioDataEvent(
            id: generateId(),
            payload: AudioPayload(audioType: "system", data: audioData),
            type: nil,
            wsEventType: "audio-data-event"
        )
        
        sendToAllWebSockets(event: event)
    }
    
    private func generateId() -> String {
        let characters = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_-"
        let length = 21
        return String((0..<length).map { _ in characters.randomElement()! })
    }
    
    private func sendToAllWebSockets<T: Codable>(event: T) {
        Task { @MainActor in
            let currentWebSockets = webSockets
            
            guard !currentWebSockets.isEmpty else { return }
            
            do {
                let encoder = JSONEncoder()
                // è®¾ç½®æœ€é«˜çš„æµ®ç‚¹æ•°ç²¾åº¦
                encoder.outputFormatting = [.withoutEscapingSlashes]
                
                let jsonData = try encoder.encode(event)
                if let jsonString = String(data: jsonData, encoding: .utf8) {
                    for webSocket in currentWebSockets {
                        try await webSocket.send(jsonString)
                    }
                }
            } catch {
                print("âŒ WebSocketå‘é€å¤±è´¥: \(error)")
            }
        }
    }
    
    func stopGlobalAudioCapture() async {
        print("ğŸ›‘ åœæ­¢éŸ³é¢‘æ•è·...")
        isCapturing = false
        
        // é¦–å…ˆå…³é—­æ‰€æœ‰ WebSocket è¿æ¥
        let socketsToClose = await MainActor.run {
            let sockets = webSockets
            webSockets.removeAll()
            return sockets
        }
        
        // å…³é—­æ‰€æœ‰WebSocketè¿æ¥
        for socket in socketsToClose {
            try? await socket.close()
        }
        
        // åœæ­¢ScreenCaptureKitæµ
        if let stream = captureStream {
            print("ğŸ›‘ åœæ­¢ScreenCaptureKitæµ...")
            do {
                try await stream.stopCapture()
                print("âœ… ScreenCaptureKitæµå·²åœæ­¢")
            } catch {
                print("âš ï¸ åœæ­¢ScreenCaptureKitæµæ—¶å‡ºé”™: \(error)")
            }
        }
        captureStream = nil
        
        // åœæ­¢éº¦å…‹é£å¼•æ“
        if let engine = micAudioEngine {
            print("ğŸ›‘ åœæ­¢éº¦å…‹é£å¼•æ“...")
            engine.inputNode.removeTap(onBus: 0)
            engine.stop()
        }
        micAudioEngine = nil
        
        print("âœ… éŸ³é¢‘æ•è·å·²å®Œå…¨åœæ­¢")
    }
    
    // MARK: - SCStreamOutput
    
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
        switch type {
        case .screen:
            break
        case .audio:
            processSystemAudioSample(sampleBuffer: sampleBuffer)
        case .microphone:
            processMicrophoneAudioSample(sampleBuffer: sampleBuffer)
        @unknown default:
            break
        }
    }
    
    // MARK: - SCStreamDelegate
    func stream(_ stream: SCStream, didStopWithError error: Error) {
        print("âŒ ScreenCaptureKitæµé”™è¯¯: \(error)")
    }
    
    // MARK: - Audio Processing
    private func processSystemAudioSample(sampleBuffer: CMSampleBuffer) {
        guard let audioBuffer = convertSampleBufferToPCMBuffer(sampleBuffer) else {
            return
        }
        processSystemAudio(buffer: audioBuffer)
    }
    
    private func processMicrophoneAudioSample(sampleBuffer: CMSampleBuffer) {
        guard let audioBuffer = convertSampleBufferToPCMBuffer(sampleBuffer) else {
            return
        }
        processMicrophoneAudio(buffer: audioBuffer)
    }
    
    private func convertSampleBufferToPCMBuffer(_ sampleBuffer: CMSampleBuffer) -> AVAudioPCMBuffer? {
        guard let formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer),
              let audioStreamBasicDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription) else {
            print("âŒ æ— æ³•è·å–éŸ³é¢‘æ ¼å¼æè¿°")
            return nil
        }
        
        guard let sourceFormat = AVAudioFormat(streamDescription: audioStreamBasicDescription) else {
            print("âŒ æ— æ³•åˆ›å»ºæºéŸ³é¢‘æ ¼å¼")
            return nil
        }
        
        let frameCount = CMSampleBufferGetNumSamples(sampleBuffer)
        
        guard let sourceBuffer = AVAudioPCMBuffer(pcmFormat: sourceFormat, frameCapacity: AVAudioFrameCount(frameCount)) else {
            print("âŒ æ— æ³•åˆ›å»ºæºPCMç¼“å†²åŒº")
            return nil
        }
        
        sourceBuffer.frameLength = AVAudioFrameCount(frameCount)
        
        if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
            var dataPointer: UnsafeMutablePointer<Int8>?
            var lengthAtOffset: Int = 0
            let status = CMBlockBufferGetDataPointer(blockBuffer, atOffset: 0, lengthAtOffsetOut: &lengthAtOffset, totalLengthOut: nil, dataPointerOut: &dataPointer)
            
            if status == noErr, let data = dataPointer {
                let audioBufferList = sourceBuffer.mutableAudioBufferList
                let bytesToCopy = min(lengthAtOffset, Int(audioBufferList.pointee.mBuffers.mDataByteSize))
                audioBufferList.pointee.mBuffers.mData?.copyMemory(from: data, byteCount: bytesToCopy)
            }
        }
        
        guard let targetFormat = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: sourceFormat.sampleRate,
            channels: sourceFormat.channelCount,
            interleaved: false
        ) else {
            print("âŒ æ— æ³•åˆ›å»ºç›®æ ‡éŸ³é¢‘æ ¼å¼")
            return nil
        }
        
        if sourceFormat.commonFormat == .pcmFormatFloat32 {
            return sourceBuffer
        }
        
        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: AVAudioFrameCount(frameCount)) else {
            print("âŒ æ— æ³•åˆ›å»ºè¾“å‡ºPCMç¼“å†²åŒº")
            return nil
        }
        
        guard let converter = AVAudioConverter(from: sourceFormat, to: targetFormat) else {
            print("âŒ æ— æ³•åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨")
            return nil
        }
        
        var error: NSError?
        let status = converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            outStatus.pointee = .haveData
            return sourceBuffer
        }
        
        if status == .error {
            return nil
        }
        return outputBuffer
    }
}

enum AudioCaptureError: Error {
    case formatError
    case permissionDenied
    case engineError
    case screenCaptureNotAvailable
} 